


#########################################
#                                       #
# Variant Calling Pipeline              #
#                                       #
#########################################



fastq-mcf:  # Non default, run with --trim-software fastq-mcf to use it
    '{executable}
     -u
     -q 20
     -P 33
     -x 10
     -l 50
     -o {forward_output}
     -o {reverse_output}
     {illumina_adapters}
     {forward_reads}
     {reverse_reads}'

cutadapt:  # Default for TrimAdapters
  '{executable}
   --error-rate=0.1
   --overlap=15
   -q 20,20
   --max-n=10
   --minimum-length=50
   -b file:{illumina_adapters_read1}
   -B file:{illumina_adapters_read2}
   --output={forward_output}
   --paired-output={reverse_output}
   {forward_reads}
   {reverse_reads}'

bwa:
    # output goes to STDOUT
    '{executable} mem
     -t {num_threads}
     -M
     -R "@RG\tID:{platform_unit}.{run_number}.{flowcell_id}.{lane_numbers_merged}\tLB:{library_id}\tPL:{platform}\tPU:{platform_unit}\tSM:{sample_id}"
     {reference_genome}
     {forward_reads}
     {reverse_reads}'

# -R adds the same read group to all reads. The idea is to fix the
# lane_numbers_merged with the specific lane_number per read downstream in the
# pipeline.
#
# I am not that sure that the -M flag is necessary, but GATK best practices
# suggests it: "The -M flag causes BWA to mark shorter split hits as
# secondary (essential for Picard compatibility)."
# https://gatkforums.broadinstitute.org/gatk/discussion/2799/howto-map-and-mark-duplicates
#
# This post doesn't throw much light on the subject:
# https://www.biostars.org/p/97323/

picard AddOrReplaceReadGroups:
    '{executable}
     AddOrReplaceReadGroups
     -I {input_sam}
     -O {output_bam}
     -SORT_ORDER coordinate
     -CREATE_INDEX True
     -VALIDATION_STRINGENCY LENIENT

     -RGID.{platform_unit}.{run_number}.{flowcell_id}.{lane_number}

     -RGSM {sample_id}
     -RGLB {library_id}
     -RGPL {platform}
     -RGPU {platform_unit}'

# See:
#
# https://gatkforums.broadinstitute.org/gatk/discussion/6472
#
# The read group ID is the "lowest denominator that differentiates factors
# contributing to technical batch effects: therefore, a read group is
# effectively treated as a separate run of the instrument in data processing
# steps such as base quality score recalibration, since they are assumed to
# share the same error model". See:
#
# Also, see:
#
# https://software.broadinstitute.org/gatk/documentation/article.php?id=6472
#
# "In the simple case where a single library preparation derived from a single
# biological sample was run on a single lane of a flowcell, all the reads from
# that lane run belong to the same read group. When multiplexing is involved,
# then each subset of reads originating from a separate library run on that
# lane will constitute a separate read group."
#
# ---
#
# I'm setting the RGID to be composed of the PLATFORM UNIT + FLOWCELL + LANE,
# no matter what the sample is. The idea is that all DNA that was run on the
# same NGS platform, machine, flowcell and lane, will share "batch effects",
# artifacts. I'm not that sure if the sample should anyway be included, in case
# each sample comes from a different library preparation? Usually our samples
# come from the same library for any single NGS run.

picard ValidateSamFile:
  '{executable}
   ValidateSamFile
   -I {input_sam}
   -O {output_txt}'

picard SortSam:
  '{executable}
   SortSam
   -I {input_sam}
   -O {output_bam}
   --SORT_ORDER coordinate'

picard MarkDuplicates:
  # The option ASSUME_SORT_ORDER should match AddOrReplaceReadGroups SORT_ORDER
  # (i.e. the previous task SORT_ORDER value)
  '{executable}
   MarkDuplicates
   -I {input_bam}
   -O {output_bam}
   --METRICS_FILE {output_metrics_file}
   --ASSUME_SORT_ORDER coordinate'

picard BuildBamIndex:
  '{executable}
   BuildBamIndex
   -I {input_bam}
   -O {output_bai}'

gatk3 HaplotypeCaller:

    # This step is the same for two pipelines:
    #
    #  - "all_sites"
    #  - "variant_sites"
    #
    #  The "target_sites" pipeline uses other settings.

    '{executable}
     -T HaplotypeCaller
     -I {input_bam}
     -gt_mode DISCOVERY
     -ERC GVCF
     -variant_index_parameter 128000
     -variant_index_type LINEAR
     -L {panel_regions}
     -R {reference_genome}
     -D {dbsnp_GRCh37}
     -o {output_gvcf}
     -bamout {output_bam}'

    # The -nt and -nct options can't be used when asking for a -bamout

    # NOTE: I'm not using the --interval_padding option because I'd rather
    # have a -L panel_regions BED file that has regions already padded +- 100
    # basepairs (Agilent support files include these).

gatk4 HaplotypeCaller:
    '{executable}
     HaplotypeCaller
     -I {input_bam}
     -O {output_gvcf}
     --intervals {panel_regions}
     --reference {reference_genome}
     --dbsnp {dbsnp_GRCh37}
     --annotation-group StandardAnnotation
     --genotyping-mode DISCOVERY
     --emit-ref-confidence GVCF
     --bam-output {output_bam}'

    # ^ Notes on gatk3 HaplotypeCaller also apply to this command

gatk3 HaplotypeCaller target_sites:

    # GENOTYPE_GIVEN_ALLELES is not very well documented in GATK's guides.
    # https://software.broadinstitute.org/gatk/documentation/article?id=2803
    #
    # -gt_mode and -alleles options to include the panel sites only
    # -L seems to be necessary; the regions include the alleles anyway
    # -ERC cannot be set when GGA is set

    '{executable}
     -T HaplotypeCaller
     -I {input_bam}
     -gt_mode GENOTYPE_GIVEN_ALLELES
     -alleles {panel_variants}
     -L {panel_regions}
     -R {reference_genome}
     -D {dbsnp_GRCh37}
     -o {output_vcf}
     -bamout {output_bam}'

gatk3 GenotypeGVCFs all_sites:
    # The {input_gvcfs} will be filled in the corresponding
    # task with -V <GVCF path> for each sample GVCF.
    '{executable}
     -T GenotypeGVCFs
     -allSites
     -R {reference_genome}
     -D {dbsnp_GRCh37}
     -nt {num_threads}
     {input_gvcfs}
     -o {output_vcf}'

# Removed -L {panel_regions}
# See the link cited above for PrintReads

gatk3 GenotypeGVCFs variant_sites:

    # The only difference between this step and the same step above,
    # of the "all_sites" pipeline, is the missing -allSites parameter.

    # The {input_gvcfs} will be filled in the corresponding
    # task with -V <GVCF path> for each sample GVCF.
    '{executable}
     -T GenotypeGVCFs
     -R {reference_genome}
     -D {dbsnp_GRCh37}
     -nt {num_threads}
     {input_gvcfs}
     -o {output_vcf}'

# Removed -L {panel_regions}
# See the link cited above for PrintReads

bcftools reset_filters:

    # This step belongs to the "target_sites" pipeline.
    #
    # It's necessary because GATK's HaplotypeCaller will mark every
    # 0/0 genotype as "LowQual", even though they have good depth and
    # quality.
    #
    # This user seems to have had the same issue, but no satisfactory answer
    # was given: http://gatkforums.broadinstitute.org/gatk/discussion/6615/many-lowqual-flags-in-output-vcf-when-using-haplotypecaller-with-alleles-arg

    '{executable}
     annotate
     --remove FILTER,QUAL
     --output-type v
     --output {output_vcf}
     {input_vcf}'

gatk3 CombineVariants:

    # The {input_vcfs} will be filled in the corresponding
    # task with -V <VCF path> for each sample VCF.

    '{executable}
     -T CombineVariants
     -nt {num_threads}
     -R {reference_genome}
     {input_vcfs}
     -o {output_vcf}'

snpsift dbSNP:

     # output goes to STDOUT

    '{executable}
     annotate
     -v
     -noAlt
     -id
     -sorted
     {dbsnp_GRCh37}
     {input_vcf}'

gatk3 SelectVariants snps:
    '{executable}
     -T SelectVariants
     -selectType SNP
     -nt {num_threads}
     -R {reference_genome}
     -V {input_vcf}
     -o {output_vcf}'

gatk3 SelectVariants indels:
    '{executable}
     -T SelectVariants
     -selectType INDEL
     -nt {num_threads}
     -R {reference_genome}
     -V {input_vcf}
     -o {output_vcf}'

gatk3 VariantFiltration snps:
    '{executable}
     -T VariantFiltration
     -R {reference_genome}
     -V {input_vcf}
     --filterName QD
     -filter "QD < 2.0"
     --filterName MQ
     -filter "MQ < 40.0"
     --filterName MQRS
     -filter "MQRankSum < -12.5"
     --filterName RPRS
     -filter "ReadPosRankSum < -8.0"

     --filterName FS
     -filter "FS > 60.0"

     -o {output_vcf}'

# Exome filters use FS > 60.0 (Macrogen people use this, for instance)
# We do not use it for panels though, so make sure you remove it for panel
# cohorts!


gatk3 VariantFiltration indels:
    '{executable}
     -T VariantFiltration
     -R {reference_genome}
     -V {input_vcf}
     --filterName QD
     -filter "QD < 2.0"
     --filterName RPRS
     -filter "ReadPosRankSum < 20.0"

     --filterName FS
     -filter "FS > 200.0"

     -o {output_vcf}'

gatk3 CombineVariants snps_indels:
    # The :snps and :indels with -priority option are mandatory
    '{executable}
     -T CombineVariants
     -nt {num_threads}
     -R {reference_genome}
     -V:snps {input_snps}
     -V:indels {input_indels}
     -genotypeMergeOptions PRIORITIZE
     -priority snps,indels
     -o {output_vcf}'

gatk3 VariantFiltration genos:
    '{executable}
     -T VariantFiltration
     -R {reference_genome}
     -V {input_vcf}
     -G_filterName LowGQ
     -G_filter "GQ < {min_gq}"
     -G_filterName LowDP
     -G_filter "DP < {min_dp}"
     -o {output_vcf}'

    # I originally included this RGQ filter for the all_sites pipeline:
    #
    #   -G_filterName LowRGQ
    #   -G_filter "RGQ < 30.0"
    #
    # However, I found that no 0/0 sites with RGQ reached this step
    # because the previous filter_snps/indels removes them (the 0/0
    # sites are not tagged as either SNP nor INDEL).
    #
    # I'll leave them commented out until a solution for that issue
    # is found.


fix contig names and sample name:
  'cat {input_vcf} |
   sed "s/ID=chr/ID=/" |
   sed "s/^chr//" |
   sed "s/{external_sample_name}/{new_sample_name}/"'
   # Macrogen VCF files use the UCSC hg19 reference genome, which has contigs
   # named "chr1", "chr2", instead of GATK bundle names like "1", "2".
   # We need to fix this to use our pipeline.


snpeff annotate:
    '{executable}
    ann
    GRCh37.75
    -canon
    -csvStats {output_summary_csv}
    -nodownload
    -datadir {snpeff_datadir}
    {input_vcf}'

    # I previously used hg19 as database (instead of GRCh37.75).
    # They should be the same.
    # -canon: Only use canonical transcripts.
    # -csvStats: this option creates an extra CSV with stats,
    # which can be later used by multiqc
    # -interval ?

vep annotate:
    # Options are described here:
    # http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html

    '{executable}
     --offline
     --dir {vep_datadir}
     --dir_cache {vep_datadir}
     --dir_plugins {vep_datadir}
     --species homo_sapiens
     --assembly GRCh37
     --fasta {reference_genome_hg19}
     --force_overwrite
     --format vcf
     --tab
     --everything
     --merged
     --stats_file {output_stats_html}
     --verbose
     -i {input_vcf}
     -o {output_tsv}'

     # {vep_datadir} is specified in ~/.paip/resources.yml
     #
     # The --merged option is important since the local
     # database for VEP is the 'homo_sapiens_merged' one.
     #
     # --format vcf refers to the *input* file format
     #
     # --tab refers to the *output* file format, it means a TSV file!
     # I'd rather put this output in a clean tab-delimited file than stuffing
     # the INFO field of the VCF with VEP's huge annotation (which makes the
     # VCF unreadable).

gatk3 SelectVariants sample:
    '{executable}
     -T SelectVariants
     -nt {num_threads}
     -R {reference_genome}
     -V {input_vcf}
     --sample_name {sample_id}
     -o {output_vcf}'


gatk3 SelectVariants reportable:
    "{executable}
     -T SelectVariants
     -nt {num_threads}
     -R {reference_genome}
     -V {input_vcf}
     --excludeFiltered
     --selectexpressions 'vc.getGenotype(\"{sample}\").getGQ() >= {min_GQ}'
     -o {output_vcf}"

# I used to include this filter:
#
# -select 'vc.getGenotype(\"{sample}\").getGQ() >= {min_GQ} && vc.getGenotype(\"{sample}\").getDP() >= {min_DP}'
#
# But I'd rather have a low depth variant be excluded manually later in the
# pipeline.

##
#
# DEPRECATED: an old part of GATK3 early versions pipeline
#
##
# gatk3 RealignerTargetCreator:
  # '{executable}
    # -T RealignerTargetCreator
    # -I {input_bam}
    # -R {reference_genome}
    # -known {indels_1000G}
    # -known {indels_mills}
    # -L {panel_regions}
    # -nt {num_threads}
    # -o {outfile}'

# gatk3 IndelRealigner:
  # '{executable}
    # -T IndelRealigner
    # -I {input_bam}
    # -R {reference_genome}
    # -targetIntervals {targets_file}
    # -o {output_bam}'

# gatk3 BaseRecalibrator:
  # '{executable}
    # -T BaseRecalibrator
    # -nct 1
    # -I {input_bam}
    # -R {reference_genome}
    # -L {panel_regions}
    # -knownSites {indels_1000G}
    # -knownSites {indels_mills}
    # -o {outfile}'

# # About PrintReads:
# # never include the -L option here: it leads to lost data! More here:
# # https://software.broadinstitute.org/gatk/documentation/article.php?id=4133
# gatk3 PrintReads:
    # '{executable}
     # -T PrintReads
     # -nct 1
     # -I {input_bam}
     # -BQSR {recalibration_table}
     # -R {reference_genome}
     # -o {output_bam}'


#########################################
#                                       #
# Quality Control and Metrics Pipeline  #
#                                       #
#########################################



multiqc:
  '{executable}
   --filename {report_filename}
   --force
   {basedir}'


fastqc:
  '{executable}
   {forward_reads}
   {reverse_reads}'


picard CollectAlignmentSummaryMetrics:
  '{executable}
   CollectAlignmentSummaryMetrics
   -I {input_bam}
   -R {reference_genome}
   -O {output_txt}'


picard CollectVariantCallingMetrics:
  '{executable}
   CollectVariantCallingMetrics
   -I {input_vcf}
   -R {reference_genome}
   -DBSNP {dbsnp_GRCh37}
   -O {output_txt}'


gatk3 VariantEval:
  '{executable}
   -T VariantEval
   --eval {input_vcf}
   -R {reference_genome}
   -D {dbsnp_GRCh37}
   -o {output_file}'


bcftools stats:
  '{executable}
   stats
   {input_vcf}'
   # Output goes to STDOUT


samtools stats:
  '{executable}
   stats
   {input_bam}'
   # Output goes to STDOUT


featureCounts:
  '{executable}
   -a {human_features}
   -o {outfile}
   -T 1
   {input_bam}'

samtools depth:
  '{executable}
   depth
   -a
   -Q 40
   -b {panel_regions}
   {input_bam}'

   # The -Q 40 makes samtools count the bases with a mapping quality > 40,
   # which is the MQ threshold that GATK uses in other part of the pipeline.

gatk3 DiagnoseTargets:
  '{executable}
   -T DiagnoseTargets
   -I {input_bam}
   --minimum_mapping_quality 40
   --minimum_coverage {min_dp}
   -R {reference_genome}
   -L {panel_regions}
   -o {output_vcf}'

gatk3 DepthOfCoverage:
  '{executable}
   -T DepthOfCoverage
   -I {input_bam}
   -dt BY_SAMPLE
   -dcov 5000
   --start 1
   --stop 5000
   --nBins 200
   --includeRefNSites
   --minMappingQuality 40
   --countType COUNT_FRAGMENTS
   -R {reference_genome}
   -L {panel_regions}
   -o {outfile}'

   # Many of this options are copied from the XHMM workflow:
   # https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml
   #
   # COUNT_FRAGMENTS instructs GATK to properly count any overlapping parts
   # of paiered-end mate pairs *only once*. This is useful for the
   # detection of CNVs, where we want to compare how deep each interval is
   # read relative to the same interval in other samples. It shouldn't be
   # used to analyze the absolute read depth and decide wether to trust
   # a genotype or not, because it considers only half the reads in the
   # overlapping part of the interval.



#########################################
#                                       #
# Variant Calling Pipeline              #
#                                       #
#########################################


# These CNV calling steps are taken from
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4065038/pdf/nihms-590058.pdf


xhmm MergeGATKDepths:
  '{executable}
  --mergeGATKdepths
  -o {outfile}
  {sample_cvg_files}'

gatk3 GCContentByInterval:
  '{executable}
   -T GCContentByInterval
   -L {panel_regions}
   -R {reference_genome}
   -o {outfile}'

awk extreme_GC_targets:
  "awk '($2 < 0.1 || $2 > 0.9) {{print $1}}' {gc_content_by_interval} > {outfile}"

  # The input file has two columns: interval name and GC content of the interval
  # This awk command keeps the interval names of the intervals with extreme
  # values (i.e. less than 10% or more than 90%) in the second column.
  # The double curly braces {{ }} around the awk print statement are necessary
  # because of Python string formatting.

xhmm centerData:
  '{executable}
   --matrix -r {read_depth_matrix}
   --centerData --centerType target
   -o {out_matrix}
   --outputExcludedTargets {out_excluded_targets}
   --outputExcludedSamples {out_excluded_samples}
   --excludeTargets {extreme_gc_targets}
   --minTargetSize 10 --maxTargetSize 1000
   --minMeanTargetRD 10 --maxMeanTargetRD 800
   --minMeanSampleRD 50 --maxMeanSampleRD 300 --maxSdSampleRD 100'

  # Default values (for EXOMES!) are these:
  #
  # --minTargetSize 10 --maxTargetSize 10000
  # --minMeanTargetRD 10 --maxMeanTargetRD 500
  # --minMeanSampleRD 25 --maxMeanSampleRD 200 --maxSdSampleRD 150
  #
  # I had to tune them heavily so that they don't leave most panel data out.
  # They should, however, be tuned for each specific NGS run, since the
  # range of read depth will vary greatly according to the NGS settings
  # and those cannot be guessed here in advance.

  # ENP parameters
  # --minTargetSize 10 --maxTargetSize 10000
  # --minMeanTargetRD 10 --maxMeanTargetRD 800
  # --minMeanSampleRD 200 --maxMeanSampleRD 800 --maxSdSampleRD 150'

xhmm PCA:
  '{executable}
   --PCA -r {filtered_centered_matrix}
   --PCAfiles {outfiles_basename}'

xhmm PCA_normalization:
  '{executable}
   --normalize -r {filtered_centered_matrix}
   --PCAfiles {pca_files_basename}
   --normalizeOutput {outfile}
   --PCnormalizeMethod PVE_mean
   --PVE_mean_factor 0.7'

xhmm Z_scores:
  '{executable}
   --matrix -r {pca_normalized_matrix}
   --centerData --centerType sample
   --zScoreData
   -o {out_zscores}
   --outputExcludedTargets {out_excluded_targets}
   --outputExcludedSamples {out_excluded_samples}
   --maxSdTargetRD 30'

xhmm filter_prenormalized_matrix:
  '{executable}
   --matrix -r {read_depth_matrix}
   --excludeTargets {raw_excluded_targets}
   --excludeTargets {zscore_excluded_targets}
   --excludeSamples {raw_excluded_samples}
   --excludeSamples {zscore_excluded_samples}
   -o {outfile}'

xhmm discover:
  '{executable}
   --discover
   -s {data_files_basename}
   -p {xhmm_params_file}
   -r {zscores_matrix}
   -R {read_depth_matrix_filtered}
   -c {outfile}
   -a {aux_outfile}'

xhmm genotype:
  '{executable}
   --genotype
   -p {xhmm_params_file}
   -r {zscores_matrix}
   -R {read_depth_matrix}
   -g {cnvs_file}
   -F {reference_genome}
   -v {output_vcf}'

Rscript make_XHMM_plots:
  '{executable} {script_path}'




#########################################
#                                       #
# Reports Pipeline                      #
#                                       #
#########################################



igv snapshots:
  'DISPLAY=:{DISPLAY}
   {executable}
   -b {script_path}'
